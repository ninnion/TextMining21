{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0710d27d57f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-37215560c2b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/julianalexander/Library/Mobile Documents/com~apple~CloudDocs/HSG/1. Semester MBI FS2021/8,049,1.00 Text Mining mit Python/TextMining21/Reddit_Julian_TOTAL-klassifiziert.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/julianalexander/Library/Mobile Documents/com~apple~CloudDocs/HSG/1. Semester MBI FS2021/8,049,1.00 Text Mining mit Python/TextMining21/Reddit_Marek_klassifiziert.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/julianalexander/Library/Mobile Documents/com~apple~CloudDocs/HSG/1. Semester MBI FS2021/8,049,1.00 Text Mining mit Python/TextMining21/Reddit_Yonas_Total_Klassifiziert.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"/Users/julianalexander/Library/Mobile Documents/com~apple~CloudDocs/HSG/1. Semester MBI FS2021/8,049,1.00 Text Mining mit Python/TextMining21/Reddit_Julian_TOTAL-klassifiziert.csv\")\n",
    "df2 = pd.read_csv(\"/Users/julianalexander/Library/Mobile Documents/com~apple~CloudDocs/HSG/1. Semester MBI FS2021/8,049,1.00 Text Mining mit Python/TextMining21/Reddit_Marek_klassifiziert.csv\")\n",
    "df3 = pd.read_csv(\"/Users/julianalexander/Library/Mobile Documents/com~apple~CloudDocs/HSG/1. Semester MBI FS2021/8,049,1.00 Text Mining mit Python/TextMining21/Reddit_Yonas_Total_Klassifiziert.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Atweets = pd.concat([df1, df2, df3], axis=0, join='inner')\n",
    "Atweets = Atweets.sort_values(by='index')\n",
    "Atweets = Atweets.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Punktuierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(txt):\n",
    "    txt_nopunt = \"\".join([c for c in txt if c not in string.punctuation])\n",
    "    return txt_nopunt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Atweets[\"submission\"] = Atweets[\"submission\"].apply(lambda x: remove_punctuation(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(txt):\n",
    "    tokens = re.split(\"\\W+\", txt)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Atweets[\"submissions_tokenized\"] = Atweets[\"submission\"].apply(lambda x: tokenize(x.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwörter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.append(\"ll\")\n",
    "stopwords.append(\"im\")\n",
    "stopwords.append(\"youre\")\n",
    "stopwords.append(\"youve\")\n",
    "stopwords.append(\"youll\")\n",
    "stopwords.append(\"youd\")\n",
    "stopwords.append(\"shes\")\n",
    "stopwords.append(\"its\")\n",
    "stopwords.append(\"thatll\")\n",
    "stopwords.append(\"dont\")\n",
    "stopwords.append(\"shouldve\")\n",
    "stopwords.append(\"arent\")\n",
    "stopwords.append(\"couldnt\")\n",
    "stopwords.append(\"didnt\")\n",
    "stopwords.append(\"doesnt\")\n",
    "stopwords.append(\"hadnt\")\n",
    "stopwords.append(\"havent\")\n",
    "stopwords.append(\"isnt\")\n",
    "stopwords.append(\"mightnt\")\n",
    "stopwords.append(\"neednt\")\n",
    "stopwords.append(\"shant\")\n",
    "stopwords.append(\"shouldnt\")\n",
    "stopwords.append(\"wasnt\")\n",
    "stopwords.append(\"werent\")\n",
    "stopwords.append(\"wont\")\n",
    "stopwords.append(\"wouldnt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(txt_tokenized):\n",
    "    txt_clean = [word for word in txt_tokenized if word not in stopwords]\n",
    "    return txt_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Atweets[\"submission_no_stopwords\"] = Atweets[\"submissions_tokenized\"].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text-Bereinigung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = \"\".join([c for c in text if c not in string.punctuation])\n",
    "    tokens = re.split(\"\\W+\", text)\n",
    "    text = [word for word in tokens if word not in stopwords]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "Atweets[\"submission_nostop\"] = Atweets[\"submission\"].apply(lambda x: clean_text(x.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(tokenized_text):\n",
    "    text = [ps.stem(word) for word in tokenized_text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "Atweets[\"submission_stemmed\"] = Atweets[\"submission_nostop\"].apply(lambda x: stemming(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(token_txt):\n",
    "    text = [wn.lemmatize(word) for word in token_txt]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "Atweets[\"submission_lemmatized\"] = Atweets[\"submission_nostop\"].apply(lambda x: lemmatization(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>submission</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>submissions_tokenized</th>\n",
       "      <th>submission_no_stopwords</th>\n",
       "      <th>submission_nostop</th>\n",
       "      <th>submission_stemmed</th>\n",
       "      <th>submission_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I see that many countries are working hard to ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[i, see, that, many, countries, are, working, ...</td>\n",
       "      <td>[see, many, countries, working, hard, publish,...</td>\n",
       "      <td>[see, many, countries, working, hard, publish,...</td>\n",
       "      <td>[see, mani, countri, work, hard, publish, news...</td>\n",
       "      <td>[see, many, country, working, hard, publish, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>httpwwwtheguardiancomtechnology2013oct29bitcoi...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[httpwwwtheguardiancomtechnology2013oct29bitco...</td>\n",
       "      <td>[httpwwwtheguardiancomtechnology2013oct29bitco...</td>\n",
       "      <td>[httpwwwtheguardiancomtechnology2013oct29bitco...</td>\n",
       "      <td>[httpwwwtheguardiancomtechnology2013oct29bitco...</td>\n",
       "      <td>[httpwwwtheguardiancomtechnology2013oct29bitco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Because I see a lot of down and out people who...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[because, i, see, a, lot, of, down, and, out, ...</td>\n",
       "      <td>[see, lot, people, fucked, loanswhat, think, r...</td>\n",
       "      <td>[see, lot, people, fucked, loanswhat, think, r...</td>\n",
       "      <td>[see, lot, peopl, fuck, loanswhat, think, rain...</td>\n",
       "      <td>[see, lot, people, fucked, loanswhat, think, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>so like many im not 100 clear on how LN functi...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[so, like, many, im, not, 100, clear, on, how,...</td>\n",
       "      <td>[like, many, 100, clear, ln, functions, granul...</td>\n",
       "      <td>[like, many, 100, clear, ln, functions, granul...</td>\n",
       "      <td>[like, mani, 100, clear, ln, function, granula...</td>\n",
       "      <td>[like, many, 100, clear, ln, function, granula...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Think i missed the Bottom  what about you\\n\\ne...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[think, i, missed, the, bottom, what, about, y...</td>\n",
       "      <td>[think, missed, bottom, edit, damn, autocorrec...</td>\n",
       "      <td>[think, missed, bottom, edit, damn, autocorrec...</td>\n",
       "      <td>[think, miss, bottom, edit, damn, autocorrect,...</td>\n",
       "      <td>[think, missed, bottom, edit, damn, autocorrec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>2594</td>\n",
       "      <td>I know there are a lot of walk throughs about ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[i, know, there, are, a, lot, of, walk, throug...</td>\n",
       "      <td>[know, lot, walk, throughs, getting, private, ...</td>\n",
       "      <td>[know, lot, walk, throughs, getting, private, ...</td>\n",
       "      <td>[know, lot, walk, through, get, privat, key, s...</td>\n",
       "      <td>[know, lot, walk, throughs, getting, private, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>2595</td>\n",
       "      <td>how is a cryptocurrency exchange different tha...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[how, is, a, cryptocurrency, exchange, differe...</td>\n",
       "      <td>[cryptocurrency, exchange, different, bank, le...</td>\n",
       "      <td>[cryptocurrency, exchange, different, bank, le...</td>\n",
       "      <td>[cryptocurr, exchang, differ, bank, less, prof...</td>\n",
       "      <td>[cryptocurrency, exchange, different, bank, le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>2596</td>\n",
       "      <td>A US federal judge rejected Alibaba Group Hold...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[a, us, federal, judge, rejected, alibaba, gro...</td>\n",
       "      <td>[us, federal, judge, rejected, alibaba, group,...</td>\n",
       "      <td>[us, federal, judge, rejected, alibaba, group,...</td>\n",
       "      <td>[us, feder, judg, reject, alibaba, group, hold...</td>\n",
       "      <td>[u, federal, judge, rejected, alibaba, group, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3001</th>\n",
       "      <td>2597</td>\n",
       "      <td>Anyone know of any Also do most online shops a...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[anyone, know, of, any, also, do, most, online...</td>\n",
       "      <td>[anyone, know, also, online, shops, accept, lo...</td>\n",
       "      <td>[anyone, know, also, online, shops, accept, lo...</td>\n",
       "      <td>[anyon, know, also, onlin, shop, accept, lot, ...</td>\n",
       "      <td>[anyone, know, also, online, shop, accept, lot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3002</th>\n",
       "      <td>2598</td>\n",
       "      <td>German children play with stacks of money dur...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[, german, children, play, with, stacks, of, m...</td>\n",
       "      <td>[, german, children, play, stacks, money, hype...</td>\n",
       "      <td>[, german, children, play, stacks, money, hype...</td>\n",
       "      <td>[, german, children, play, stack, money, hyper...</td>\n",
       "      <td>[, german, child, play, stack, money, hyperinf...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3003 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                         submission  sentiment  \\\n",
       "0         0  I see that many countries are working hard to ...        2.0   \n",
       "1         0  httpwwwtheguardiancomtechnology2013oct29bitcoi...        3.0   \n",
       "2         1  Because I see a lot of down and out people who...        2.0   \n",
       "3         1  so like many im not 100 clear on how LN functi...        3.0   \n",
       "4         2  Think i missed the Bottom  what about you\\n\\ne...        3.0   \n",
       "...     ...                                                ...        ...   \n",
       "2998   2594  I know there are a lot of walk throughs about ...        3.0   \n",
       "2999   2595  how is a cryptocurrency exchange different tha...        3.0   \n",
       "3000   2596  A US federal judge rejected Alibaba Group Hold...        3.0   \n",
       "3001   2597  Anyone know of any Also do most online shops a...        3.0   \n",
       "3002   2598   German children play with stacks of money dur...        3.0   \n",
       "\n",
       "                                  submissions_tokenized  \\\n",
       "0     [i, see, that, many, countries, are, working, ...   \n",
       "1     [httpwwwtheguardiancomtechnology2013oct29bitco...   \n",
       "2     [because, i, see, a, lot, of, down, and, out, ...   \n",
       "3     [so, like, many, im, not, 100, clear, on, how,...   \n",
       "4     [think, i, missed, the, bottom, what, about, y...   \n",
       "...                                                 ...   \n",
       "2998  [i, know, there, are, a, lot, of, walk, throug...   \n",
       "2999  [how, is, a, cryptocurrency, exchange, differe...   \n",
       "3000  [a, us, federal, judge, rejected, alibaba, gro...   \n",
       "3001  [anyone, know, of, any, also, do, most, online...   \n",
       "3002  [, german, children, play, with, stacks, of, m...   \n",
       "\n",
       "                                submission_no_stopwords  \\\n",
       "0     [see, many, countries, working, hard, publish,...   \n",
       "1     [httpwwwtheguardiancomtechnology2013oct29bitco...   \n",
       "2     [see, lot, people, fucked, loanswhat, think, r...   \n",
       "3     [like, many, 100, clear, ln, functions, granul...   \n",
       "4     [think, missed, bottom, edit, damn, autocorrec...   \n",
       "...                                                 ...   \n",
       "2998  [know, lot, walk, throughs, getting, private, ...   \n",
       "2999  [cryptocurrency, exchange, different, bank, le...   \n",
       "3000  [us, federal, judge, rejected, alibaba, group,...   \n",
       "3001  [anyone, know, also, online, shops, accept, lo...   \n",
       "3002  [, german, children, play, stacks, money, hype...   \n",
       "\n",
       "                                      submission_nostop  \\\n",
       "0     [see, many, countries, working, hard, publish,...   \n",
       "1     [httpwwwtheguardiancomtechnology2013oct29bitco...   \n",
       "2     [see, lot, people, fucked, loanswhat, think, r...   \n",
       "3     [like, many, 100, clear, ln, functions, granul...   \n",
       "4     [think, missed, bottom, edit, damn, autocorrec...   \n",
       "...                                                 ...   \n",
       "2998  [know, lot, walk, throughs, getting, private, ...   \n",
       "2999  [cryptocurrency, exchange, different, bank, le...   \n",
       "3000  [us, federal, judge, rejected, alibaba, group,...   \n",
       "3001  [anyone, know, also, online, shops, accept, lo...   \n",
       "3002  [, german, children, play, stacks, money, hype...   \n",
       "\n",
       "                                     submission_stemmed  \\\n",
       "0     [see, mani, countri, work, hard, publish, news...   \n",
       "1     [httpwwwtheguardiancomtechnology2013oct29bitco...   \n",
       "2     [see, lot, peopl, fuck, loanswhat, think, rain...   \n",
       "3     [like, mani, 100, clear, ln, function, granula...   \n",
       "4     [think, miss, bottom, edit, damn, autocorrect,...   \n",
       "...                                                 ...   \n",
       "2998  [know, lot, walk, through, get, privat, key, s...   \n",
       "2999  [cryptocurr, exchang, differ, bank, less, prof...   \n",
       "3000  [us, feder, judg, reject, alibaba, group, hold...   \n",
       "3001  [anyon, know, also, onlin, shop, accept, lot, ...   \n",
       "3002  [, german, children, play, stack, money, hyper...   \n",
       "\n",
       "                                  submission_lemmatized  \n",
       "0     [see, many, country, working, hard, publish, n...  \n",
       "1     [httpwwwtheguardiancomtechnology2013oct29bitco...  \n",
       "2     [see, lot, people, fucked, loanswhat, think, r...  \n",
       "3     [like, many, 100, clear, ln, function, granula...  \n",
       "4     [think, missed, bottom, edit, damn, autocorrec...  \n",
       "...                                                 ...  \n",
       "2998  [know, lot, walk, throughs, getting, private, ...  \n",
       "2999  [cryptocurrency, exchange, different, bank, le...  \n",
       "3000  [u, federal, judge, rejected, alibaba, group, ...  \n",
       "3001  [anyone, know, also, online, shop, accept, lot...  \n",
       "3002  [, german, child, play, stack, money, hyperinf...  \n",
       "\n",
       "[3003 rows x 8 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Atweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
